{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96748cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER THE SENTENCE TO BE PREPROCESSED: <p>In the <b>bustling city</b> of New York, <i>people rush</i> through their daily routines.</p> <p>They often forget to <a href=\"https://example.com\">pause and appreciate</a> the beauty around them.</p> <p>The city is known for its iconic landmarks, such as the <u>Statue of Liberty</u> and Central Park.</p>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "s = input(\"ENTER THE SENTENCE TO BE PREPROCESSED: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42c0115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>In the <b>bustling city</b> of New York, <i>people rush</i> through their daily routines.</p> <p>They often forget to <a href=\"https://example.com\">pause and appreciate</a> the beauty around them.</p> <p>The city is known for its iconic landmarks, such as the <u>Statue of Liberty</u> and Central Park.</p>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e067cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SENTENCES IS:-  1\n",
      "NUMBER OF WORDS IS:-  91\n",
      "AVERAGE NUMBER OF WORDS PER SENTNCE:-  91\n",
      "UNIQUE NUMBER OF WORDS:-  54\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sents = nltk.sent_tokenize(s)\n",
    "print(\"NUMBER OF SENTENCES IS:- \",len(sents))\n",
    "words = nltk.word_tokenize(s)\n",
    "print(\"NUMBER OF WORDS IS:- \",len(words))\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"AVERAGE NUMBER OF WORDS PER SENTNCE:- \",average_tokens)\n",
    "unique_tokens = set(words)\n",
    "print(\"UNIQUE NUMBER OF WORDS:- \",len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a0eff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGGED WORDS:- \n",
      "[('<', 'JJ'), ('p', 'NN'), ('>', 'NN'), ('In', 'IN'), ('the', 'DT'), ('<', 'NN'), ('b', 'NN'), ('>', 'NNP'), ('bustling', 'VBG'), ('city', 'NN'), ('<', 'NNP'), ('/b', 'NNP'), ('>', 'NNP'), ('of', 'IN'), ('New', 'NNP'), ('York', 'NNP'), (',', ','), ('<', 'NNP'), ('i', 'VBZ'), ('>', 'VBP'), ('people', 'NNS'), ('rush', 'VBP'), ('<', 'JJ'), ('/i', 'NN'), ('>', 'NNP'), ('through', 'IN'), ('their', 'PRP$'), ('daily', 'JJ'), ('routines.', 'NN'), ('<', 'NNP'), ('/p', 'NNP'), ('>', 'NNP'), ('<', 'NNP'), ('p', 'VBP'), ('>', 'NNP'), ('They', 'PRP'), ('often', 'RB'), ('forget', 'VBP'), ('to', 'TO'), ('<', 'VB'), ('a', 'DT'), ('href=', 'NN'), (\"''\", \"''\"), ('https', 'NN'), (':', ':'), ('//example.com', 'NN'), (\"''\", \"''\"), ('>', 'NN'), ('pause', 'NN'), ('and', 'CC'), ('appreciate', 'NN'), ('<', 'NNS'), ('/a', 'VBP'), ('>', 'IN'), ('the', 'DT'), ('beauty', 'NN'), ('around', 'IN'), ('them.', 'JJ'), ('<', 'NNP'), ('/p', 'NNP'), ('>', 'NNP'), ('<', 'NNP'), ('p', 'NN'), ('>', 'VBD'), ('The', 'DT'), ('city', 'NN'), ('is', 'VBZ'), ('known', 'VBN'), ('for', 'IN'), ('its', 'PRP$'), ('iconic', 'JJ'), ('landmarks', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('<', 'NNP'), ('u', 'JJ'), ('>', 'NNP'), ('Statue', 'NNP'), ('of', 'IN'), ('Liberty', 'NNP'), ('<', 'NNP'), ('/u', 'NNP'), ('>', 'NNP'), ('and', 'CC'), ('Central', 'NNP'), ('Park.', 'NNP'), ('<', 'NNP'), ('/p', 'NNP'), ('>', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Postagging\n",
    "from nltk import pos_tag\n",
    "tagged = pos_tag(words)\n",
    "print(\"TAGGED WORDS:- \")\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0edbfaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS AFTER STOPWORD REMOVAL:-  ['<', 'p', '>', 'In', '<', 'b', '>', 'bustling', 'city', '<', '/b', '>', 'New', 'York', ',', '<', '>', 'people', 'rush', '<', '/i', '>', 'daily', 'routines.', '<', '/p', '>', '<', 'p', '>', 'They', 'often', 'forget', '<', 'href=', \"''\", 'https', ':', '//example.com', \"''\", '>', 'pause', 'appreciate', '<', '/a', '>', 'beauty', 'around', 'them.', '<', '/p', '>', '<', 'p', '>', 'The', 'city', 'known', 'iconic', 'landmarks', ',', '<', 'u', '>', 'Statue', 'Liberty', '<', '/u', '>', 'Central', 'Park.', '<', '/p', '>']\n",
      "NUMBER OF WORDS AFTER STOPWORD REMOVAL:-  74\n"
     ]
    }
   ],
   "source": [
    "#Stopwords Removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('English'))\n",
    "stopwordsfree_tokens = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stopwordsfree_tokens.append(word)\n",
    "print(\"WORDS AFTER STOPWORD REMOVAL:- \",stopwordsfree_tokens)  \n",
    "print(\"NUMBER OF WORDS AFTER STOPWORD REMOVAL:- \",len(stopwordsfree_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05eb7573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------><\n",
      "p------>p\n",
      ">------>>\n",
      "In------>in\n",
      "<------><\n",
      "b------>b\n",
      ">------>>\n",
      "bustling------>bustl\n",
      "city------>citi\n",
      "<------><\n",
      "/b------>/b\n",
      ">------>>\n",
      "New------>new\n",
      "York------>york\n",
      ",------>,\n",
      "<------><\n",
      ">------>>\n",
      "people------>peopl\n",
      "rush------>rush\n",
      "<------><\n",
      "/i------>/i\n",
      ">------>>\n",
      "daily------>daili\n",
      "routines.------>routines.\n",
      "<------><\n",
      "/p------>/p\n",
      ">------>>\n",
      "<------><\n",
      "p------>p\n",
      ">------>>\n",
      "They------>they\n",
      "often------>often\n",
      "forget------>forget\n",
      "<------><\n",
      "href=------>href=\n",
      "''------>''\n",
      "https------>http\n",
      ":------>:\n",
      "//example.com------>//example.com\n",
      "''------>''\n",
      ">------>>\n",
      "pause------>paus\n",
      "appreciate------>appreci\n",
      "<------><\n",
      "/a------>/a\n",
      ">------>>\n",
      "beauty------>beauti\n",
      "around------>around\n",
      "them.------>them.\n",
      "<------><\n",
      "/p------>/p\n",
      ">------>>\n",
      "<------><\n",
      "p------>p\n",
      ">------>>\n",
      "The------>the\n",
      "city------>citi\n",
      "known------>known\n",
      "iconic------>icon\n",
      "landmarks------>landmark\n",
      ",------>,\n",
      "<------><\n",
      "u------>u\n",
      ">------>>\n",
      "Statue------>statu\n",
      "Liberty------>liberti\n",
      "<------><\n",
      "/u------>/u\n",
      ">------>>\n",
      "Central------>central\n",
      "Park.------>park.\n",
      "<------><\n",
      "/p------>/p\n",
      ">------>>\n"
     ]
    }
   ],
   "source": [
    "#STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmed_word = []\n",
    "p_stemmer = PorterStemmer()\n",
    "for word in stopwordsfree_tokens:\n",
    "    print(word + '------>' + p_stemmer.stem(word))\n",
    "    stemmed_word.append(p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b9c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'p', '>', 'in', '<', 'b', '>', 'bustl', 'citi', '<', '/b', '>', 'new', 'york', ',', '<', '>', 'peopl', 'rush', '<', '/i', '>', 'daili', 'routines.', '<', '/p', '>', '<', 'p', '>', 'they', 'often', 'forget', '<', 'href=', \"''\", 'http', ':', '//example.com', \"''\", '>', 'paus', 'appreci', '<', '/a', '>', 'beauti', 'around', 'them.', '<', '/p', '>', '<', 'p', '>', 'the', 'citi', 'known', 'icon', 'landmark', ',', '<', 'u', '>', 'statu', 'liberti', '<', '/u', '>', 'central', 'park.', '<', '/p', '>']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d81ca72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'in', 'b', 'bustl', 'citi', '/b', 'new', 'york', 'peopl', 'rush', '/i', 'daili', 'routines.', '/p', 'p', 'they', 'often', 'forget', 'href=', \"''\", 'http', '//example.com', \"''\", 'paus', 'appreci', '/a', 'beauti', 'around', 'them.', '/p', 'p', 'the', 'citi', 'known', 'icon', 'landmark', 'u', 'statu', 'liberti', '/u', 'central', 'park.', '/p']\n"
     ]
    }
   ],
   "source": [
    "#FILTERING\n",
    "import string\n",
    "filtered_word = []\n",
    "for word in stemmed_word:\n",
    "    if word not in string.punctuation:\n",
    "        filtered_word.append(word)\n",
    "print(filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d68bd",
   "metadata": {},
   "source": [
    "#SCRIPT VALIDATION\n",
    "import re\n",
    "def r(text):\n",
    "    res = re.sub(r'<.*?>','',text)\n",
    "    return res\n",
    "\n",
    "final_data = r(filtered_word)\n",
    "\n",
    "print(\"FINAL PREPROCESSED DATA:- \",final_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d852d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
